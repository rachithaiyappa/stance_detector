{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24cf273",
   "metadata": {},
   "source": [
    "### This notebook demonstrates \n",
    "1. How to build prompts for a particular target, \n",
    "2. How to build context free prompts for a particular target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9494d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stance_detector.prompt_builder import PromptBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217258e2",
   "metadata": {},
   "source": [
    "# SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37620c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:11:12,701 - stance_detector.prompt_builder - INFO - Loaded CSV file: ../data/semeval_taskA.csv\n"
     ]
    }
   ],
   "source": [
    "builder = PromptBuilder(\"../data/semeval_taskA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd813264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:11:12,726 - stance_detector.prompt_builder - INFO - Filtered DataFrame for all targets: ['Atheism']\n",
      "2025-05-14 14:11:12,727 - stance_detector.prompt_builder - INFO - Selected prompt templates: ['1']\n",
      "2025-05-14 14:11:12,728 - stance_detector.prompt_builder - INFO - Selected instruction templates: ['1']\n",
      "2025-05-14 14:11:12,745 - stance_detector.prompt_builder - INFO - Prompts built successfully with tweets included.\n",
      "2025-05-14 14:11:12,748 - stance_detector.prompt_builder - INFO - Saving prompts to: ../data/semeval_taskA_targetAtheism_prompt1_instruction1_prompts.parquet\n"
     ]
    }
   ],
   "source": [
    "df = builder.build_all_prompts(\n",
    "    output_path = \"../data/semeval_taskA_targetAtheism_prompt1_instruction1_prompts.parquet\",\n",
    "    targets=\"Atheism\",\n",
    "    prompt_template_key=\"1\",\n",
    "    instruction_key=\"1\",\n",
    "    )\n",
    "\n",
    "# Note: if any one of targets, prompt_template_key, or instruction is missing, \n",
    "# the package builds the prompts for all possible entries of the the missing fields.\n",
    "# Try the following.\n",
    "# df = builder.build_all_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c78f4912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>label</th>\n",
       "      <th>class_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>He who exalts himself shall      be humbled; a...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Your response to the question should be either...</td>\n",
       "      <td>1--1</td>\n",
       "      <td>[Positive, Negative, Neutral, Positive., Negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Your response to the question should be either...</td>\n",
       "      <td>1--1</td>\n",
       "      <td>[Positive, Negative, Neutral, Positive., Negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Your response to the question should be either...</td>\n",
       "      <td>1--1</td>\n",
       "      <td>[Positive, Negative, Neutral, Positive., Negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>#God is utterly powerless without Human interv...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Your response to the question should be either...</td>\n",
       "      <td>1--1</td>\n",
       "      <td>[Positive, Negative, Neutral, Positive., Negat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>Atheism</td>\n",
       "      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Your response to the question should be either...</td>\n",
       "      <td>1--1</td>\n",
       "      <td>[Positive, Negative, Neutral, Positive., Negat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID   Target                                              Tweet   Stance  \\\n",
       "0  10001  Atheism  He who exalts himself shall      be humbled; a...  AGAINST   \n",
       "1  10002  Atheism  RT @prayerbullets: I remove Nehushtan -previou...  AGAINST   \n",
       "2  10003  Atheism  @Brainman365 @heidtjj @BenjaminLives I have so...  AGAINST   \n",
       "3  10004  Atheism  #God is utterly powerless without Human interv...  AGAINST   \n",
       "4  10005  Atheism  @David_Cameron   Miracles of #Multiculturalism...  AGAINST   \n",
       "\n",
       "                                              Prompt label  \\\n",
       "0  Your response to the question should be either...  1--1   \n",
       "1  Your response to the question should be either...  1--1   \n",
       "2  Your response to the question should be either...  1--1   \n",
       "3  Your response to the question should be either...  1--1   \n",
       "4  Your response to the question should be either...  1--1   \n",
       "\n",
       "                                        class_tokens  \n",
       "0  [Positive, Negative, Neutral, Positive., Negat...  \n",
       "1  [Positive, Negative, Neutral, Positive., Negat...  \n",
       "2  [Positive, Negative, Neutral, Positive., Negat...  \n",
       "3  [Positive, Negative, Neutral, Positive., Negat...  \n",
       "4  [Positive, Negative, Neutral, Positive., Negat...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc659209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response to the question should be either positive, negative, or neutral.\n",
      "\n",
      "Statement: He who exalts himself shall      be humbled; and he who humbles himself shall be exalted.Matt 23:12.     #SemST\n",
      "Question: What is the attitude of the statement to Atheism?\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "print(df.Prompt.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94752e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Negative', 'Neutral', 'Positive.', 'Negative.', 'Neutral.', 'positive', 'negative', 'neutral', 'positive.', 'negative.', 'neutral.']\n"
     ]
    }
   ],
   "source": [
    "# The LLM output probabilities (given the prompt) will be measured for these tokens\n",
    "print(df.class_tokens.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a39cb0",
   "metadata": {},
   "source": [
    "# SemEval context free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e877767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:11:12,932 - stance_detector.prompt_builder - INFO - Filtered DataFrame for all targets: ['Atheism']\n",
      "2025-05-14 14:11:12,933 - stance_detector.prompt_builder - INFO - Selected prompt templates: ['1']\n",
      "2025-05-14 14:11:12,934 - stance_detector.prompt_builder - INFO - Selected instruction templates: ['1']\n",
      "2025-05-14 14:11:12,937 - stance_detector.prompt_builder - INFO - Context-free prompts built successfully. Tweets omitted from prompts.\n",
      "2025-05-14 14:11:12,940 - stance_detector.prompt_builder - INFO - Saving prompts to: ../data/semeval_taskA_targetAtheism_prompt1_instruction1_prompts_free.parquet\n"
     ]
    }
   ],
   "source": [
    "df_cf = builder.build_all_prompts(\n",
    "    output_path = \"../data/semeval_taskA_targetAtheism_prompt1_instruction1_prompts_free.parquet\",\n",
    "    targets=\"Atheism\",\n",
    "    prompt_template_key=\"1\",\n",
    "    instruction_key=\"1\",\n",
    "    context_free=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb15f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response to the question should be either positive, negative, or neutral.\n",
      "\n",
      "Statement: \n",
      "Question: What is the attitude of the statement to Atheism?\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "print(df_cf.Prompt.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d3621",
   "metadata": {},
   "source": [
    "# P-Stance\n",
    "\n",
    "Notice how the class tokens/prompt dont include a third option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049f3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:11:13,013 - stance_detector.prompt_builder - INFO - Loaded CSV file: ../data/pstance_bernie.csv\n"
     ]
    }
   ],
   "source": [
    "builder = PromptBuilder(\"../data/pstance_bernie.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51661122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:11:13,038 - stance_detector.prompt_builder - WARNING - No targets provided, using all available targets.\n",
      "2025-05-14 14:11:13,038 - stance_detector.prompt_builder - INFO - Filtered DataFrame for all targets\n",
      "2025-05-14 14:11:13,039 - stance_detector.prompt_builder - INFO - Selected prompt templates: ['3a']\n",
      "2025-05-14 14:11:13,039 - stance_detector.prompt_builder - INFO - Selected instruction templates: ['3']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:11:13,088 - stance_detector.prompt_builder - INFO - Prompts built successfully with tweets included.\n",
      "2025-05-14 14:11:13,092 - stance_detector.prompt_builder - INFO - Saving prompts to: ../data/pstance_bernie_targetBernie:Sanders_prompt3a_instruction3_prompts.parquet\n"
     ]
    }
   ],
   "source": [
    "df = builder.build_all_prompts(\n",
    "    output_path = \"../data/pstance_targetbernie_prompt1_instruction1_prompts.parquet\",\n",
    "    prompt_template_key=\"3a\",\n",
    "    instruction_key=\"3\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47bac228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kindly provide your answer to the question in the format of true, or false.\n",
      "\n",
      "Statement: #IEndorseBernie for tons of reasons, but this one is my number 1. We are running out of time to save our planet from the worst of climate change. We NEED Bernies #GreenNewDeal. #Bernie @Lafayette, Indiana\n",
      "Question: The statement is against Bernie Sanders.\n"
     ]
    }
   ],
   "source": [
    "print(df.Prompt.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d012ad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['True', 'False', 'True.', 'False.', 'true', 'false', 'true.', 'false.']\n"
     ]
    }
   ],
   "source": [
    "print(df.class_tokens.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265588a",
   "metadata": {},
   "source": [
    "# P-Stance context free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca244a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 14:11:13,171 - stance_detector.prompt_builder - WARNING - No targets provided, using all available targets.\n",
      "2025-05-14 14:11:13,172 - stance_detector.prompt_builder - INFO - Filtered DataFrame for all targets\n",
      "2025-05-14 14:11:13,172 - stance_detector.prompt_builder - INFO - Selected prompt templates: ['2']\n",
      "2025-05-14 14:11:13,173 - stance_detector.prompt_builder - INFO - Selected instruction templates: ['2']\n",
      "2025-05-14 14:11:13,175 - stance_detector.prompt_builder - INFO - Context-free prompts built successfully. Tweets omitted from prompts.\n",
      "2025-05-14 14:11:13,178 - stance_detector.prompt_builder - INFO - Saving prompts to: ../data/pstance_bernie_targetBernie:Sanders_prompt2_instruction2_prompts_free.parquet\n"
     ]
    }
   ],
   "source": [
    "df_cf = builder.build_all_prompts(\n",
    "    output_path = \"../data/pstance_targetbernie_prompt2_instruction2_prompts_free.parquet\",\n",
    "    prompt_template_key=\"2\",\n",
    "    instruction_key=\"2\",\n",
    "    context_free=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b4297f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question needs to be answered with either favor, or against.\n",
      "\n",
      "Statement: \n",
      "Question: What is the stance of the statement to Bernie Sanders?\n"
     ]
    }
   ],
   "source": [
    "print(df_cf.Prompt.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
